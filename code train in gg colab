import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# ÄÆ°á»ng dáº«n dá»¯ liá»‡u
data_dir = '/content/drive/MyDrive/AI successfully'

# ================== Cáº¤U HÃŒNH THAM Sá» ==================
IMG_SIZE = 224  # KÃ­ch thÆ°á»›c áº£nh Ä‘áº§u vÃ o
BATCH_SIZE = 32  # KÃ­ch thÆ°á»›c batch
EPOCHS_INITIAL = 15  # Sá»‘ epochs cho giai Ä‘oáº¡n Ä‘áº§u
EPOCHS_FINETUNE = 35  # Sá»‘ epochs cho giai Ä‘oáº¡n fine-tune
LEARNING_RATE_INITIAL = 1e-4  # Learning rate ban Ä‘áº§u
LEARNING_RATE_FINETUNE = 5e-5  # Learning rate cho fine-tune
# ======================================================

# ================== DATA AUGMENTATION ==================
# Data augmentation máº¡nh máº½ hÆ¡n cho training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,  # 20% lÃ m validation
    rotation_range=40,     # Xoay áº£nh nhiá»u hÆ¡n
    width_shift_range=0.25,
    height_shift_range=0.25,
    shear_range=0.2,       # ThÃªm biáº¿n dáº¡ng gÃ³c
    zoom_range=0.3,        # Zoom in/out nhiá»u hÆ¡n
    horizontal_flip=True,
    vertical_flip=True,    # ThÃªm láº­t dá»c
    brightness_range=[0.8, 1.2],  # Thay Ä‘á»•i Ä‘á»™ sÃ¡ng
    channel_shift_range=0.2,      # Thay Ä‘á»•i cÃ¡c kÃªnh mÃ u
    fill_mode='nearest'
)

# Chá»‰ rescale cho validation
val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Táº¡o generators
print("ğŸ”„ Äang táº¡o generators cho dá»¯ liá»‡u...")
train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

validation_generator = val_datagen.flow_from_directory(
    data_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# In ra cÃ¡c classes vÃ  sá»‘ lÆ°á»£ng áº£nh
class_indices = train_generator.class_indices
class_names = list(class_indices.keys())
print(f"ğŸ·ï¸ CÃ¡c lá»›p nháº­n dáº¡ng: {class_names}")

# ================== TÃNH CLASS WEIGHTS ==================
# TÃ­nh class weights Ä‘á»ƒ xá»­ lÃ½ máº¥t cÃ¢n báº±ng dá»¯ liá»‡u
class_counts = [0] * len(class_names)
for subdir in os.listdir(data_dir):
    subdir_path = os.path.join(data_dir, subdir)
    if os.path.isdir(subdir_path) and subdir in class_indices:
        class_counts[class_indices[subdir]] = len([f for f in os.listdir(subdir_path)
                                                if os.path.isfile(os.path.join(subdir_path, f))])

print(f"ğŸ“Š Sá»‘ lÆ°á»£ng áº£nh má»—i lá»›p: {dict(zip(class_names, class_counts))}")

# TÃ­nh class weights tá»± Ä‘á»™ng
total_samples = sum(class_counts)
class_weights = {i: total_samples / (len(class_counts) * count) if count > 0 else 1.0
                for i, count in enumerate(class_counts)}
print(f"âš–ï¸ Class weights: {class_weights}")

# ================== Äá»ŠNH NGHÄ¨A MÃ” HÃŒNH ==================
def create_model(trainable=False, lr=LEARNING_RATE_INITIAL):
    """Táº¡o vÃ  compile mÃ´ hÃ¬nh MobileNetV2 vá»›i cÃ¡c cÃ i Ä‘áº·t tÃ¹y chá»‰nh"""

    # Load MobileNetV2 pre-trained
    base_model = MobileNetV2(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        weights='imagenet'
    )

    # Äáº·t tráº¡ng thÃ¡i trainable cho base model
    base_model.trainable = trainable

    # Náº¿u fine-tuning, chá»‰ train cÃ¡c lá»›p cuá»‘i
    if trainable:
        # ÄÃ³ng bÄƒng cÃ¡c layer Ä‘áº§u, má»Ÿ khÃ³a cÃ¡c layer cuá»‘i
        fine_tune_at = len(base_model.layers) - 30  # Má»Ÿ khÃ³a ~30 lá»›p cuá»‘i
        for layer in base_model.layers[:fine_tune_at]:
            layer.trainable = False

        print(f"ğŸ”“ ÄÃ£ má»Ÿ khÃ³a {len(base_model.layers) - fine_tune_at} lá»›p cuá»‘i cá»§a MobileNetV2 Ä‘á»ƒ fine-tune")

    # XÃ¢y dá»±ng model head phá»©c táº¡p hÆ¡n
    x = base_model.output
    x = GlobalAveragePooling2D()(x)

    # ThÃªm cÃ¡c lá»›p fully connected vá»›i batch normalization vÃ  dropout
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)

    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    # Lá»›p output vá»›i softmax
    predictions = Dense(len(class_names), activation='softmax')(x)

    # Táº¡o model
    model = Model(inputs=base_model.input, outputs=predictions)

    # Compile vá»›i optimizer Adam
    model.compile(
        optimizer=Adam(learning_rate=lr),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# ================== CALLBACKS ==================
# Táº¡o cÃ¡c callbacks
checkpoint_path = '/content/drive/MyDrive/waste_classifier_best.h5'
callbacks = [
    # LÆ°u model tá»‘t nháº¥t
    ModelCheckpoint(
        checkpoint_path,
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    # Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
    EarlyStopping(
        monitor='val_accuracy',
        patience=8,
        restore_best_weights=True,
        verbose=1
    ),
    # Giáº£m learning rate khi plateau
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=3,
        min_lr=1e-6,
        verbose=1
    )
]

# ================== TRAINING - PHASE 1 ==================
print("\n" + "="*50)
print("PHASE 1: TRAINING ONLY TOP LAYERS")
print("="*50)

# Táº¡o model vá»›i base layers Ä‘Ã³ng bÄƒng
model = create_model(trainable=False, lr=LEARNING_RATE_INITIAL)
print("ğŸ“Š Tá»•ng quan mÃ´ hÃ¬nh:")
model.summary()

# Training phase 1
print("\nğŸš€ Báº¯t Ä‘áº§u training phase 1...")
history_initial = model.fit(
    train_generator,
    epochs=EPOCHS_INITIAL,
    validation_data=validation_generator,
    callbacks=callbacks,
    class_weight=class_weights,
    verbose=1
)

# ================== TRAINING - PHASE 2 (FINE-TUNING) ==================
print("\n" + "="*50)
print("PHASE 2: FINE-TUNING THE MODEL")
print("="*50)

# Táº¡o model phase 2 vá»›i fine-tuning layers cuá»‘i
model = create_model(trainable=True, lr=LEARNING_RATE_FINETUNE)

# Training phase 2 (fine-tuning)
print("\nğŸš€ Báº¯t Ä‘áº§u fine-tuning...")
history_finetune = model.fit(
    train_generator,
    epochs=EPOCHS_FINETUNE,
    validation_data=validation_generator,
    callbacks=callbacks,
    class_weight=class_weights,
    initial_epoch=len(history_initial.history['accuracy']),  # Tiáº¿p tá»¥c tá»« epoch cuá»‘i cá»§a phase 1
    verbose=1
)

# Gá»™p lá»‹ch sá»­ training
history = {
    'accuracy': history_initial.history['accuracy'] + history_finetune.history['accuracy'],
    'val_accuracy': history_initial.history['val_accuracy'] + history_finetune.history['val_accuracy'],
    'loss': history_initial.history['loss'] + history_finetune.history['loss'],
    'val_loss': history_initial.history['val_loss'] + history_finetune.history['val_loss']
}

# ================== ÄÃNH GIÃ MÃ” HÃŒNH ==================
# Load model tá»‘t nháº¥t
model.load_weights(checkpoint_path)

# ÄÃ¡nh giÃ¡ trÃªn táº­p validation
print("\n" + "="*50)
print("EVALUATING MODEL")
print("="*50)

# Dá»± Ä‘oÃ¡n
validation_generator.reset()
Y_pred = model.predict(validation_generator, verbose=1)
y_pred = np.argmax(Y_pred, axis=1)
y_true = validation_generator.classes

# Váº½ confusion matrix
print("\nğŸ” Äang váº½ confusion matrix...")
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Dá»± Ä‘oÃ¡n')
plt.ylabel('Thá»±c táº¿')
plt.title('Confusion Matrix')
plt.savefig('/content/drive/MyDrive/confusion_matrix.png')
plt.show()

# In classification report
print("\nğŸ“Š Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

# Váº½ learning curves
print("\nğŸ“ˆ Äang váº½ learning curves...")
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history['accuracy'], label='Training Accuracy')
plt.plot(history['val_accuracy'], label='Validation Accuracy')
plt.axvline(x=EPOCHS_INITIAL, color='r', linestyle='--', label='Start Fine-tuning')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history['loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.axvline(x=EPOCHS_INITIAL, color='r', linestyle='--', label='Start Fine-tuning')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/learning_curves.png')
plt.show()

# ================== LÆ¯U MÃ” HÃŒNH CUá»I CÃ™NG ==================
model_save_path = '/content/drive/MyDrive/waste_3classes_mobilenetv2_improved.h5'
model.save(model_save_path)
print(f"\nâœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh MobileNetV2 cáº£i tiáº¿n táº¡i: {model_save_path}")

# ================== PHÃ‚N TÃCH Lá»–I ==================
# TÃ¬m cÃ¡c trÆ°á»ng há»£p dá»± Ä‘oÃ¡n sai
print("\nğŸ” PhÃ¢n tÃ­ch má»™t sá»‘ trÆ°á»ng há»£p dá»± Ä‘oÃ¡n sai...")

errors = np.where(y_pred != y_true)[0]
if len(errors) > 0:
    print(f"Sá»‘ lÆ°á»£ng dá»± Ä‘oÃ¡n sai: {len(errors)}/{len(y_true)} ({len(errors)/len(y_true)*100:.2f}%)")

    # Chá»n ngáº«u nhiÃªn tá»‘i Ä‘a 5 trÆ°á»ng há»£p lá»—i Ä‘á»ƒ hiá»ƒn thá»‹
    import random
    sample_errors = random.sample(list(errors), min(5, len(errors)))

    # Láº¥y tÃªn file vÃ  label thá»±c táº¿/dá»± Ä‘oÃ¡n
    filenames = [validation_generator.filenames[i] for i in sample_errors]
    true_labels = [class_names[y_true[i]] for i in sample_errors]
    pred_labels = [class_names[y_pred[i]] for i in sample_errors]

    for i, (filename, true_label, pred_label) in enumerate(zip(filenames, true_labels, pred_labels)):
        print(f"Lá»—i #{i+1}: File '{filename}' - Thá»±c táº¿: {true_label}, Dá»± Ä‘oÃ¡n: {pred_label}")
else:
    print("KhÃ´ng cÃ³ lá»—i dá»± Ä‘oÃ¡n trÃªn táº­p validation!")

print("\n" + "="*50)
print("ğŸ‰ TRAINING HOÃ€N THÃ€NH!")
print("="*50)
print(f"ğŸ¯ Accuracy train cuá»‘i cÃ¹ng: {history['accuracy'][-1]*100:.2f}%")
print(f"ğŸ¯ Accuracy validation cuá»‘i cÃ¹ng: {history['val_accuracy'][-1]*100:.2f}%")
print(f"ğŸ“ Káº¿t quáº£ chi tiáº¿t Ä‘Ã£ Ä‘Æ°á»£c lÆ°u thÃ nh confusion_matrix.png vÃ  learning_curves.png")
print(f"ğŸ’¾ MÃ´ hÃ¬nh tá»‘t nháº¥t Ä‘Æ°á»£c lÆ°u táº¡i {checkpoint_path}")
print(f"ğŸ’¾ MÃ´ hÃ¬nh cuá»‘i cÃ¹ng Ä‘Æ°á»£c lÆ°u táº¡i {model_save_path}")
